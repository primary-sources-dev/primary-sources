[
  {
    "slug": "assertion-based-tracking-launch",
    "title": "Introducing Assertion Based Tracking: The Core Methodology",
    "excerpt": "How we're replacing 'facts' with source-traced assertions to enable contradiction detection, confidence scoring, and full evidentiary traceability in historical research.",
    "category": "Tools",
    "date": "2026-02-26",
    "author": "Engineering Team",
    "featured": true,
    "hero_image": "",
    "tags": ["assertion-tracking", "methodology", "architecture", "core-feature"],
    "content": [
      {
        "type": "callout",
        "variant": "info",
        "title": "Core Feature Announcement",
        "content": "This post introduces the foundational methodology that powers the entire Primary Sources platform. Everything we build—entity profiles, event timelines, document analysis—is built on assertion-based tracking."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Problem with Traditional Historical Databases"
      },
      {
        "type": "text",
        "content": "Most historical archives and databases store information as immutable facts. Wikipedia says \"Lee Harvey Oswald shot President Kennedy.\" Traditional databases record \"Event: Assassination, Date: 1963-11-22, Perpetrator: Oswald.\" But history isn't that simple."
      },
      {
        "type": "text",
        "content": "What happens when the Warren Commission concludes one thing but the House Select Committee on Assassinations concludes another? What happens when Agent Kesler's FBI report says Ralph Yates picked up a hitchhiker on November 20th, but Yates told police it was November 21st? In traditional systems, one version wins, the other is lost, and researchers lose access to the full evidentiary record."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Assertion-Based Solution"
      },
      {
        "type": "text",
        "content": "Assertion Based Tracking solves this by storing claims (assertions) instead of facts. Every statement in the database is explicitly linked to its source document, page number, and context. When contradictions exist, both assertions are preserved with confidence scores and support markers."
      },
      {
        "type": "quote",
        "content": "We don't store truth. We store evidence. The difference is everything.",
        "author": "Design Philosophy"
      },
      {
        "type": "text",
        "content": "This methodology transforms unstructured archival documents into a queryable knowledge graph where every claim can be traced to its source, conflicting accounts are explicitly marked, and researchers can instantly see the full evidentiary chain."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "How It Works: Six-Step Workflow"
      },
      {
        "type": "text",
        "content": "The assertion tracking system processes documents through a structured pipeline:"
      },
      {
        "type": "list",
        "ordered": true,
        "items": [
          "Document Upload → OCR — Archival documents processed through OCR pipeline, classified by type (FBI 302, CIA cable, testimony)",
          "Entity Extraction — Named entity recognition identifies people, places, organizations, objects, and events",
          "Assertion Creation — Claims extracted as subject-predicate-object triples: 'Yates (subject) PICKED_UP (predicate) Hitchhiker (object)'",
          "Source Linking — Each assertion linked to exact source excerpt (document ID, page number, paragraph, timestamp)",
          "Conflict Detection — System compares assertions about same subject-predicate pair, flags contradictions with confidence scores",
          "Review & Export — Researchers query assertions, review contradictions, trace evidence chains, export citations"
        ]
      },
      {
        "type": "callout",
        "variant": "tip",
        "title": "Automatic Contradiction Detection",
        "content": "When the system finds multiple assertions about the same subject-predicate-object relationship with different values or timestamps, it automatically flags them for researcher review. No more hunting through footnotes to find conflicting accounts."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Technical Architecture"
      },
      {
        "type": "text",
        "content": "At the core is a subject-predicate-object model inspired by semantic web standards and adapted for historiographic rigor. Each assertion follows this structure:"
      },
      {
        "type": "callout",
        "variant": "success",
        "title": "Assertion Schema",
        "content": "subject_type + subject_id → predicate → object_type + object_id\n\nExample: person:ralph-yates → PICKED_UP → person:hitchhiker\n\nEach assertion includes: confidence score (CONFIRMED, SUPPORTED, DISPUTED, UNLIKELY), timestamp with precision level (UNKNOWN, APPROX, EXACT, RANGE), and source excerpts array linking to specific document pages."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Real-World Example: The Yates Incident"
      },
      {
        "type": "text",
        "content": "On November 20 or 21, 1963 (depending on which document you read), Dallas resident Ralph Leon Yates picked up a hitchhiker who made disturbing comments about shooting the President. This single event generates multiple conflicting assertions:"
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "FBI Agent Kesler's report: \"Yates stated he picked up hitchhiker on November 20, 1963\" (SUPPORTED)",
          "Dallas Police interview: \"Yates told officers the incident occurred on November 21, 1963\" (SUPPORTED)",
          "Yates later testimony: \"It was sometime around Thanksgiving, maybe the 20th or 21st\" (APPROX)",
          "Warren Commission conclusion: \"No evidence hitchhiker was Oswald\" (DISPUTED by later HSCA findings)"
        ]
      },
      {
        "type": "text",
        "content": "In a traditional database, one date would be selected and the others discarded. In our assertion-based system, all three claims are preserved with their source citations, confidence scores, and the contradiction is explicitly marked. Researchers can see the full evidence and make their own determinations."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Six Core Features"
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "Source Traceability — Every assertion links to exact source excerpt with document page and timestamp",
          "Contradiction Detection — System automatically identifies conflicting assertions about the same subject",
          "Entity Consolidation — Multiple mentions of same entity consolidated to single record with fuzzy matching",
          "Confidence Scoring — Assertions rated based on source credibility and corroboration",
          "Time Precision Modeling — Timestamps tagged UNKNOWN, APPROX, EXACT, or RANGE to prevent false precision",
          "Controlled Vocabulary — Event types, predicates, roles use standardized codes for consistent querying"
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Current Status and Next Steps"
      },
      {
        "type": "text",
        "content": "The assertion tracking methodology is currently implemented as a JSON-based prototype with nested data structures. This allows us to validate the conceptual model and test the UI components before migrating to the production database."
      },
      {
        "type": "callout",
        "variant": "warning",
        "title": "In Development",
        "content": "The feature is currently in prototype phase. Database schema is complete (PostgreSQL/Supabase with 27 tables), but the query interface and API integration are still in development. Follow the blog for updates on production launch."
      },
      {
        "type": "text",
        "content": "Next phases include database integration with Supabase/PostgreSQL (27-table schema already designed), query builder interface for researchers, bulk assertion import tools, and citation export functionality."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Why This Matters"
      },
      {
        "type": "text",
        "content": "Assertion Based Tracking isn't just a technical feature—it's a fundamental shift in how we approach historical evidence. By preserving conflicting accounts with full source traceability, we enable researchers to see the complete evidentiary record, not just the version that won out."
      },
      {
        "type": "quote",
        "content": "History is messy. Evidence conflicts. Witnesses contradict each other. Our system embraces that complexity instead of hiding it.",
        "author": "Research Philosophy"
      },
      {
        "type": "text",
        "content": "This methodology forms the foundation for every feature we build. Entity profiles show assertions about each person. Event timelines display conflicting accounts side-by-side. Document pages link directly to the assertions they support. It all starts here."
      }
    ],
    "related": ["ocr-engine-launch", "document-classifier-launch", "pdf-viewer-launch"]
  },
  {
    "slug": "ocr-engine-launch",
    "title": "OCR Engine: High-Precision Text Extraction for Archival Records",
    "excerpt": "How we built an OCR pipeline optimized for typewritten archival documents with surgical page splitting, batch processing, and local-first architecture.",
    "category": "Tools",
    "date": "2026-02-25",
    "author": "Engineering Team",
    "featured": false,
    "hero_image": "",
    "tags": ["ocr", "document-processing", "research-tools", "toolbox"],
    "content": [
      {
        "type": "callout",
        "variant": "info",
        "title": "Part of the Research Toolbox",
        "content": "The OCR Engine is the first step in the assertion-based tracking pipeline. It transforms scanned archival documents into searchable text that feeds entity extraction and assertion creation."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Problem with Traditional OCR"
      },
      {
        "type": "text",
        "content": "Archival documents from the 1960s present unique challenges: typewritten text with carbon copies, degraded scans with compression artifacts, mixed layouts with handwritten annotations, and multi-page reports that need surgical splitting for targeted analysis."
      },
      {
        "type": "text",
        "content": "Generic OCR tools like Adobe Acrobat or Google Cloud Vision are optimized for modern printed text, not historical typewritten documents. They struggle with faded carbon copies, misalign text blocks when pages have stamps or annotations, and lack the surgical page-splitting features researchers need."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Solution: Optimized OCR Pipeline"
      },
      {
        "type": "text",
        "content": "We built an OCR engine specifically tuned for 1960s typewritten documents using Tesseract OCR with custom preprocessing. The system handles batch processing, supports surgical page splitting, and runs entirely locally for sensitive archival materials."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "How It Works"
      },
      {
        "type": "list",
        "ordered": true,
        "items": [
          "Document Upload — Supports PDF, JPEG, PNG, TIFF, WEBP formats. Drag-and-drop or batch upload via ZIP archives.",
          "Preprocessing — Image enhancement with contrast adjustment, noise reduction, and deskewing for misaligned scans.",
          "Tesseract OCR — Runs locally with optimized settings for typewritten text recognition. No cloud services, no data exposure.",
          "Text Extraction — Outputs plain text with coordinate mapping for deep sync with original document pages.",
          "Page Splitting — Surgical extraction of specific pages or ranges for targeted processing without reprocessing entire PDFs.",
          "Export — JSON output with text, confidence scores, and bounding box coordinates for downstream tools."
        ]
      },
      {
        "type": "callout",
        "variant": "tip",
        "title": "Batch Processing",
        "content": "Upload entire ZIP archives of scanned documents. The engine processes them sequentially, generates individual text files, and packages results for download. Process hundreds of pages overnight without manual intervention."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Key Features"
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "Local Processing — Tesseract runs on your machine. Sensitive archival materials never leave your system.",
          "Surgical Page Splitting — Extract pages 1-5, 10-12, or specific ranges without reprocessing entire documents.",
          "Batch Upload — ZIP archive support for processing dozens or hundreds of documents at once.",
          "Coordinate Mapping — Text extraction includes bounding box coordinates for deep sync with original pages.",
          "Multi-Format Support — PDF, JPEG, PNG, TIFF, WEBP, HEIC. Works with archival scans and modern mobile photos.",
          "Progress Tracking — Real-time progress bar shows page-by-page processing status for long-running jobs."
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Technical Details"
      },
      {
        "type": "text",
        "content": "The OCR engine uses Tesseract 4.x with custom training data for 1960s typewritten fonts. Preprocessing pipeline includes OpenCV-based image enhancement, deskewing algorithms for rotated scans, and adaptive thresholding for degraded carbon copies. Output format is JSON with text, confidence scores, and coordinate arrays."
      },
      {
        "type": "callout",
        "variant": "success",
        "title": "Active Tool",
        "content": "The OCR Engine is production-ready with a full web UI. Launch from the Research Toolbox at tools/ocr/ocr-ui.html. Supports real-time processing and batch jobs."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Connection to Assertion Based Tracking"
      },
      {
        "type": "text",
        "content": "OCR is the entry point for assertion-based tracking. Scanned FBI reports, CIA cables, and testimony transcripts become searchable text. That text feeds the Document Classifier for categorization, then Entity Matcher for person/place/org recognition, and finally Assertion Creation for subject-predicate-object triples. Without OCR, archival documents remain locked in image format."
      },
      {
        "type": "quote",
        "content": "OCR transforms historical artifacts into queryable data. It's the bridge from physical archives to digital knowledge graphs.",
        "author": "Research Philosophy"
      }
    ],
    "related": ["assertion-based-tracking-launch", "document-classifier-launch", "pdf-viewer-launch"]
  },
  {
    "slug": "document-classifier-launch",
    "title": "Document Classifier: Intelligent Categorization for Archival Research",
    "excerpt": "Automated document classification with 4-tier taxonomy, fuzzy fingerprinting, and metadata extraction. Organizes FBI reports, CIA cables, and testimony transcripts for systematic analysis.",
    "category": "Tools",
    "date": "2026-02-24",
    "author": "Engineering Team",
    "featured": false,
    "hero_image": "",
    "tags": ["classification", "document-analysis", "research-tools", "toolbox"],
    "content": [
      {
        "type": "callout",
        "variant": "info",
        "title": "Part of the Research Toolbox",
        "content": "The Document Classifier sits between OCR and entity extraction. It categorizes documents by type (FBI 302, CIA cable, testimony), extracts metadata (dates, authors, agencies), and prepares structured data for assertion creation."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Problem with Manual Classification"
      },
      {
        "type": "text",
        "content": "Archival collections contain thousands of documents across dozens of types: FBI 302 interview reports, CIA cables, Warren Commission testimony, HSCA hearing transcripts, agency memos, NARA reference sheets. Manually categorizing each document is time-consuming and inconsistent."
      },
      {
        "type": "text",
        "content": "Traditional keyword-based classification fails on degraded OCR text. When 'FBI' becomes 'FBl' or '302' becomes '3O2', simple string matching breaks down. Researchers waste hours manually sorting documents that could be automatically classified."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Solution: Fuzzy Fingerprinting"
      },
      {
        "type": "text",
        "content": "We built a classification system using fuzzy fingerprinting with Levenshtein distance. Instead of exact keyword matches, the system looks for structural patterns: header layouts, footer formats, text zones. It handles garbled OCR by measuring string similarity rather than requiring perfect matches."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "How It Works"
      },
      {
        "type": "list",
        "ordered": true,
        "items": [
          "Document Upload — Load PDF with OCR text layer or run OCR first if needed.",
          "Zone Detection — Identifies header, footer, and body zones using layout analysis.",
          "Fingerprint Extraction — Extracts structural patterns from each zone (header format, footer placement, text density).",
          "Fuzzy Matching — Compares extracted fingerprints against known document types using Levenshtein distance.",
          "Metadata Extraction — Pulls agency codes, dates, authors, file numbers from recognized headers/footers.",
          "Classification Output — Returns document type, confidence score, and extracted metadata in JSON format."
        ]
      },
      {
        "type": "callout",
        "variant": "tip",
        "title": "78.2% Classification Rate",
        "content": "Validated across Warren Commission, HSCA, Church Committee, and CIA 201 collections. The fuzzy fingerprinting approach achieves ~2.3x improvement over exact keyword matching on degraded scans."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Key Features"
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "20 Document Types — FBI 302, CIA Cable, Warren Commission Exhibit, HSCA Testimony, Church Committee Report, NARA Reference, and 14 more.",
          "Fuzzy Fingerprinting — Levenshtein distance handles garbled OCR. 'FBl' still matches 'FBI', '3O2' matches '302'.",
          "Zone-Specific Parsing — Extracts metadata from headers, footers, and body zones separately for higher accuracy.",
          "Multi-Collection Support — Trained on Warren Commission, HSCA, Church Committee, CIA 201 files.",
          "Confidence Scoring — Returns match confidence (0-100%) so researchers can review low-confidence classifications.",
          "Batch Processing — Classify entire document collections overnight with automated metadata extraction."
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Technical Details"
      },
      {
        "type": "text",
        "content": "The classifier uses a fingerprint database of structural patterns compiled from manually reviewed exemplars. Each document type has 3-5 fingerprint variants to handle layout variations. Zone detection uses heuristic rules for common header/footer patterns (top 15% of page, bottom 10% of page). Levenshtein threshold is set at 80% similarity for fuzzy matching."
      },
      {
        "type": "callout",
        "variant": "success",
        "title": "Active Tool",
        "content": "The Document Classifier is production-ready with a full review UI. Launch from the Research Toolbox at tools/classifier/classifier-ui.html. Supports real-time classification and manual override for edge cases."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Connection to Assertion Based Tracking"
      },
      {
        "type": "text",
        "content": "Classification is critical for assertion creation. Knowing a document is an 'FBI 302 Interview' tells the system to look for interview date, agent name, witness name, and testimony statements. A 'CIA Cable' has sender/receiver agencies and transmission dates. Document type drives entity extraction patterns and assertion templates."
      },
      {
        "type": "quote",
        "content": "Classification transforms unstructured documents into structured inputs. It's the organizing principle that makes bulk assertion creation possible.",
        "author": "Research Philosophy"
      }
    ],
    "related": ["assertion-based-tracking-launch", "ocr-engine-launch", "document-analyzer-launch"]
  },
  {
    "slug": "mission-statement",
    "title": "Our Mission: Preserving History Through Digital Archives",
    "excerpt": "Why we're building a comprehensive digital archive of primary source documents and what makes this project different.",
    "category": "About",
    "date": "2026-02-24",
    "author": "Primary Sources Team",
    "featured": true,
    "hero_image": "",
    "tags": ["mission", "about", "vision"],
    "content": [
      {
        "type": "heading",
        "level": 2,
        "content": "The Archive Vision"
      },
      {
        "type": "text",
        "content": "Primary Sources exists to make historical documents accessible, understandable, and interconnected. We believe that primary source materials—the original documents, testimonies, photographs, and records—are essential for understanding complex historical events."
      },
      {
        "type": "callout",
        "variant": "info",
        "title": "What Makes Us Different",
        "content": "Unlike traditional archives, we connect documents across collections, build contextual relationships between events and people, and present information through multiple entry points—whether you're researching a specific person, event, or document."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Core Principles"
      },
      {
        "type": "quote",
        "content": "Documents don't speak for themselves. Context, relationships, and careful presentation transform raw materials into accessible knowledge.",
        "author": "Archive Philosophy"
      },
      {
        "type": "text",
        "content": "Our work is guided by three core principles that shape every design decision and feature we build:"
      },
      {
        "type": "list",
        "ordered": true,
        "items": [
          "Accessibility First — Complex historical materials should be approachable for researchers at all levels",
          "Context Matters — Documents need surrounding information to be properly understood",
          "Connections Reveal Truth — Linking people, events, and documents exposes patterns and relationships",
          "Source Integrity — We preserve and present original documents without alteration",
          "Progressive Disclosure — Information should be layered from overview to deep detail"
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "content": "What We're Building"
      },
      {
        "type": "text",
        "content": "The archive combines traditional archival practices with modern web technology. Our component-based architecture allows us to present the same information through multiple views—event timelines, person profiles, document collections—while maintaining consistency and accuracy."
      },
      {
        "type": "callout",
        "variant": "tip",
        "title": "Technical Approach",
        "content": "We use modular component design patterns that make templates reusable and portable. Every template works as standalone HTML first, then migrates cleanly to modern frameworks like Next.js when needed."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Road Ahead"
      },
      {
        "type": "text",
        "content": "This project is iterative by design. We build features incrementally, test with real data, and refine based on how well they serve researchers. The blog you're reading now follows the same pattern—designed to be simple, replicable, and easy to port to future platforms."
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "Expand document collection with systematic scanning",
          "Build relationship mapping tools between events and people",
          "Integrate timeline visualizations for complex sequences",
          "Create search and discovery features for cross-collection research",
          "Develop citation tools for academic use"
        ]
      },
      {
        "type": "callout",
        "variant": "success",
        "title": "Join the Journey",
        "content": "This archive is a long-term commitment to preserving and presenting history accurately. Follow our development blog for updates on new features, document collections, and technical milestones."
      }
    ],
    "related": ["welcome-to-development", "design-philosophy"]
  },
  {
    "slug": "welcome-to-development",
    "title": "Development Blog Launch",
    "excerpt": "Introducing the Primary Sources development blog—a space for project updates, technical insights, and research progress.",
    "category": "Updates",
    "date": "2026-02-23",
    "author": "Development Team",
    "featured": false,
    "hero_image": "",
    "tags": ["announcement", "blog", "updates"],
    "content": [
      {
        "type": "text",
        "content": "Welcome to the Primary Sources development blog. This space will document our journey building a comprehensive digital archive of historical materials."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "What You'll Find Here"
      },
      {
        "type": "text",
        "content": "We'll be sharing regular updates across several categories:"
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "Development updates on new features and technical improvements",
          "Research insights from working with archival materials",
          "Design decisions explaining our approach to presenting complex information",
          "Project milestones and progress reports"
        ]
      },
      {
        "type": "callout",
        "variant": "info",
        "title": "Subscribe for Updates",
        "content": "Follow along as we build tools for researchers, historians, and anyone interested in exploring primary source documents through modern interfaces."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Built for Portability"
      },
      {
        "type": "text",
        "content": "This blog itself demonstrates our technical philosophy. The modular content block system you're reading right now is designed to work as static HTML today and migrate seamlessly to Next.js and MDX tomorrow."
      },
      {
        "type": "quote",
        "content": "Good architecture makes future migrations easy, not impossible.",
        "author": "Project Principle"
      }
    ],
    "related": ["mission-statement", "design-philosophy"]
  },
  {
    "slug": "design-philosophy",
    "title": "Design Philosophy: Archival Aesthetic Meets Modern Web",
    "excerpt": "How we balance the gravitas of archival materials with the usability expectations of modern web interfaces.",
    "category": "Development",
    "date": "2026-02-22",
    "author": "Design Team",
    "featured": false,
    "hero_image": "",
    "tags": ["design", "architecture", "ui"],
    "content": [
      {
        "type": "heading",
        "level": 2,
        "content": "The Challenge"
      },
      {
        "type": "text",
        "content": "Presenting historical documents requires balancing two competing needs: the weight and seriousness of archival materials, and the clarity and usability modern web users expect."
      },
      {
        "type": "callout",
        "variant": "warning",
        "title": "Design Tension",
        "content": "Too academic and you lose general audiences. Too casual and you diminish the gravity of the materials. The sweet spot is 'accessible authority.'"
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Our Approach"
      },
      {
        "type": "text",
        "content": "We developed what we call the 'archival aesthetic'—a design language that honors the seriousness of historical materials while maintaining modern usability standards."
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "Muted color palette with strategic accent colors",
          "Archive card layouts that reference physical archive boxes",
          "Uppercase typography for headers suggesting official documents",
          "Border-left accents creating visual hierarchy",
          "Grayscale imagery with hover color for engagement",
          "Monospace fonts referencing typewritten documents"
        ]
      },
      {
        "type": "heading",
        "level": 3,
        "content": "Component Architecture"
      },
      {
        "type": "text",
        "content": "Every template uses a component card library system. Cards appear only when relevant data exists, automatically expand based on content density, and maintain consistent styling across person profiles, event timelines, and document collections."
      },
      {
        "type": "quote",
        "content": "Design systems succeed when they're flexible enough to handle edge cases but consistent enough to feel unified.",
        "author": "Design Principle"
      },
      {
        "type": "callout",
        "variant": "success",
        "title": "Result",
        "content": "The system works for comprehensive profiles with dozens of data points and simple entries with just basic information. Progressive disclosure through accordions lets users dive as deep as they want."
      }
    ],
    "related": ["mission-statement", "component-library-explained"]
  },
  {
    "slug": "component-library-explained",
    "title": "Building a Universal Component Card Library",
    "excerpt": "Deep dive into our reusable card system that powers person profiles, event timelines, and now blog posts.",
    "category": "Development",
    "date": "2026-02-21",
    "author": "Engineering Team",
    "featured": false,
    "hero_image": "",
    "tags": ["technical", "components", "architecture"],
    "content": [
      {
        "type": "heading",
        "level": 2,
        "content": "The Problem"
      },
      {
        "type": "text",
        "content": "Early prototypes had separate HTML templates for each type of content—person profiles, events, documents. This led to design drift, duplicated code, and maintenance nightmares."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "The Solution"
      },
      {
        "type": "text",
        "content": "We built a universal component card library. Each card type is defined once with clear responsibilities:"
      },
      {
        "type": "list",
        "ordered": true,
        "items": [
          "Card Registry — Centralized configuration mapping card IDs to behavior",
          "Show Logic — Each card defines when it should appear based on data",
          "Auto-Expand Rules — Cards can auto-expand based on content density",
          "Populate Functions — Modular rendering functions for each card type"
        ]
      },
      {
        "type": "callout",
        "variant": "tip",
        "title": "Code Example",
        "content": "A card registry entry defines everything: icon, title, data field, visibility condition, and render function. The template evaluates these rules dynamically."
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Real-World Benefits"
      },
      {
        "type": "text",
        "content": "This architecture solved multiple problems simultaneously:"
      },
      {
        "type": "list",
        "ordered": false,
        "items": [
          "Works for comprehensive datasets AND minimal entries",
          "Empty states handle gracefully (no cards shown if no data)",
          "Adding new card types requires one registry entry",
          "Design changes propagate automatically across all templates",
          "Migration to frameworks like Next.js is straightforward"
        ]
      },
      {
        "type": "quote",
        "content": "The best architecture makes the common case trivial and the complex case possible.",
        "author": "Engineering Philosophy"
      },
      {
        "type": "heading",
        "level": 2,
        "content": "Next Steps"
      },
      {
        "type": "text",
        "content": "We're applying this same pattern to the blog system you're reading right now. Content blocks work like cards—modular, reusable, and defined once. This makes adding new block types (like video embeds or interactive timelines) as simple as adding a new renderer function."
      }
    ],
    "related": ["design-philosophy", "mission-statement"]
  }
]
